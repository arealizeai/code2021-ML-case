{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction notebook for machine learning\n",
    "The purpose of this notebook is to provide the reader with a insight into how one can analyze and train a machine learning model for the area prediction case. However, it is not mandatory to follow the steps in this notebook, it is just a resource with suggestiongs for the reader.\n",
    "\n",
    "The notebook will explain the following concepts:\n",
    "* [Importing the data](#Importing-the-data)\n",
    "* [Understanding the data](#Understanding-the-data)\n",
    "* [Cleaning the data](#Cleaning-the-data)\n",
    "* Feature engineering\n",
    "* Training a machine learning model\n",
    "* Evaluate the machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case\n",
    "*A more detailed description of the case can be found in the case document.*\n",
    "\n",
    "The teams are given a dataset with customer requirements, such as number of employees, number of meetingrooms, etc. The goal is to predict the total area for the building given these parameters and present the results in the end of the competition. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "If the teams have any questions feel free to come and talk with us. It can be anything from coding help, discussion about possible solution, or to ask us about the weather.\n",
    "\n",
    "![A bad meme encouraging you to ask us questions](./memes/ask_us_questions.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We start by importing all necessary packages. These can be installed by using the pip command by running \n",
    "\n",
    "```\n",
    "pip install -r .\\path\\to\\requirements.txt\n",
    "``` \n",
    "\n",
    "If you have not used pip before I would suggest reading this article: https://datatofish.com/install-package-python-using-pip/\n",
    "\n",
    "Feel free to use any package you find suiting for your solution. These are just some recommendation from our side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd             # for data analysis\n",
    "import numpy as np              # for number manipulations\n",
    "import matplotlib.pyplot as plt # for making graphs\n",
    "import seaborn as sns           # for making the graphs look nice\n",
    "import sklearn                  # for machine learning models\n",
    "from sklearn.model_selection import train_test_split # for splitting into train and test datasets\n",
    "\n",
    "np.random.seed(1) #remove the randomness\n",
    "sns.set_style('darkgrid') #https://seaborn.pydata.org/generated/seaborn.set_style.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data\n",
    "The dataset used for training and testing will be provided in a CSV format. One can use pandas functionality to import the data as a dataframe.\n",
    "\n",
    "```\n",
    "dataset = pd.read_csv(\".\\path\\to\\dataset.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\".\\path\\to\\dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting into training and testing dataset**\n",
    "\n",
    "One should split the dataset into two different datasets, one for training and one for testing. The reason one should have a testing dataset is to validate the machine learning model on data that the model had **not seen before**.\n",
    "\n",
    "However, it might be benefical to do this stage later as it will be easier to do feature engineering one the whole dataset, and then splitt it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset, testing_dataset = train_test_split(dataset, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the data\n",
    "An important part before training a machine learning model is to understand the data. This will give you a better understanding on which models and methods is best suited for the given problem, and what kinds of modifications are required on the dataset.\n",
    "\n",
    "This stage is often called exploritory data analysis and is a part of the data preprosessing. In this notebook a basic overview of the data will be given. For a good example on EDA on the famous titanic dataset one should read [Ashwini Swain's notebook over on kaggle](https://www.kaggle.com/ash316/eda-to-prediction-dietanic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
