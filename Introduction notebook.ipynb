{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction notebook for machine learning\n",
    "The purpose of this notebook is to provide the reader with a insight into how one can analyze and train a machine learning model for the area prediction case. However, it is not mandatory to follow the steps in this notebook, it is just a resource with suggestiongs for the reader.\n",
    "\n",
    "The notebook will explain the following concepts:\n",
    "* [Importing the data](#Importing-the-data)\n",
    "* [Understanding the data](#Understanding-the-data)\n",
    "* [Feature engineering & data cleaning](#Feature-engineering-&-data-cleaning)\n",
    "* [Training a machine learning model](#Training-a-machine-learning-model)\n",
    "* [Evaluate the machine learning model](#Evaluate-the-machine-learning-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case\n",
    "*A more detailed description of the case can be found in the case document.*\n",
    "\n",
    "The teams are given a dataset with customer requirements, such as number of employees, number of meeting rooms, etc. The goal is to predict the total area for the building given these parameters and present the results in the end of the competition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "If the teams have any questions feel free to come and talk with us. It can be anything from coding help, discussion about possible solution, or to ask us about the weather.\n",
    "\n",
    "![A bad meme encouraging you to ask us questions](./img/ask_us_questions.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the dataset\n",
    "This notebook uses a earlier version of the dataset than what the teams will get. The dataset in this notebook only has Oslo as a city, but the one the teams will be handed out will have Bergen and Trondheim as well. Moreover, some of the features have changed name. I.E. netto_area has been changed to sqm_nett_area.\n",
    "\n",
    "However, this notebook will still be a good introduction to the concept of how you can do machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We start by importing all necessary packages. These can be installed by using the pip command by running \n",
    "\n",
    "```\n",
    "pip install -r .\\path\\to\\requirements.txt\n",
    "``` \n",
    "\n",
    "If you have not used pip before I would suggest reading this article: https://datatofish.com/install-package-python-using-pip/\n",
    "\n",
    "Feel free to use any package you find suiting for your solution. These are just some recommendation from our side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd             # for data analysis\n",
    "import numpy as np              # for number manipulations\n",
    "import matplotlib.pyplot as plt # for making graphs\n",
    "import seaborn as sns           # for making the graphs look nice\n",
    "import sklearn                  # for machine learning models\n",
    "from sklearn.model_selection import train_test_split # for splitting into train and test datasets\n",
    "\n",
    "np.random.seed(1) #remove the randomness\n",
    "sns.set_style('darkgrid') #https://seaborn.pydata.org/generated/seaborn.set_style.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data\n",
    "The dataset used for training and testing will be provided in a CSV format. One can use pandas functionality to import the data as a dataframe.\n",
    "\n",
    "```\n",
    "dataset = pd.read_csv(\".\\path\\to\\dataset.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"./dataset_complete.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting into training and testing dataset**\n",
    "\n",
    "One should split the dataset into two different datasets, one for training and one for testing. The reason one should have a testing dataset is to validate the machine learning model on data that the model had **not seen before**.\n",
    "\n",
    "However, it might be benefical to do this stage later as it will be easier to do feature engineering one the whole dataset, and then splitt it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset, testing_dataset = train_test_split(dataset, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the data\n",
    "An important part before training a machine learning model is to understand the data. This will give you a better understanding on which models and methods is best suited for the given problem, and what kinds of modifications are required on the dataset.\n",
    "\n",
    "This stage is often called exploritory data analysis and is a part of the data preprosessing. In this notebook a basic overview of the data will be given. For a good example on EDA on the famous titanic dataset one should read [Ashwini Swain's notebook over on kaggle](https://www.kaggle.com/ash316/eda-to-prediction-dietanic).\n",
    "\n",
    "**Note:** In this notebook a not finished dataset has been used, therefore some of the attributes will reflect the missing fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do we see?**\n",
    "\n",
    "From the describe we can see that there are 501 datapoints currently (see count under id). We also see that none of the fields have missing values, since all of them have a count of 501.\n",
    "\n",
    "There are two suspicious max values, sqm_social_zones and netto_area. We see that the max netto_area is 549, which is almost dobble the 75%. We also see that on sqm_social_zones, that the max is more than dobble of 75%. Both of these does not have to mean anything, but they can also be outliers that should be removed.\n",
    "\n",
    "From the ```dataset.head()``` we see that industry and city is categorical values, this means they are not numbers but rather classes they fit into. For a linear regression model, these have to either be removed or changed to a numberic value.\n",
    "\n",
    "We also see that the dataset has a unique id for each datapoint, these should be removed as this will prevent the machine learning model overfitting the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.heatmap(dataset.corr(), annot=True, cmap='RdYlGn', linewidths=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Heatmap**\n",
    "\n",
    "A heatmap is a useful tool to visualize corrulation between different attributes in a dataset. In this example a [pearson correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) matrix has been calculated, which means it will calculate a score between -1 and 1 on how linearly corrulated the attributes are. Both -1 and 1 means a perfect linear corrulation.\n",
    "\n",
    "We can see from the heatmap that all attribute has a high linear corrulation with netto_area. However, we also see that sqm_social_zones almost has a 1 in correlation coefficient, so this will be an important feature for a linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Industry\n",
    "In this subsection, the industry attribute of the dataset will be explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=dataset, x=\"industry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do we see?**\n",
    "\n",
    "We see that there are two industries when the notebook was created. We also see that there is almost a 50-50 splitt between tech and finance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=dataset[(dataset['industry'] == 'tech')], x=\"sqm_net_area\")\n",
    "sns.histplot(data=dataset[(dataset['industry'] == 'finance')], x=\"sqm_net_area\", color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do we see?**\n",
    "\n",
    "We see that both the tech and finance industry has a similar distrubution of netto_area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### City\n",
    "In this subsection, the city attribute of the dataset will be explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['city'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do we see?**\n",
    "\n",
    "When this notebook was created, there was only one city in the dataset, Oslo. However, the teams will be handed out a dataset with multiple city and the teams have to see how relevant this attribute is for them. As of now this attribute can be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sqm_social_zone\n",
    "In this subsection, the square meter of socal zone will be explored. We already see from the heatmap that this will have a high correlation with netto_area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=dataset, x='sqm_social_zones', y='sqm_net_area', marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=dataset, x='sqm_social_zones', y='sqm_net_area', hue='industry', marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do we see?**\n",
    "\n",
    "We can see that there are a linear correlation between netto_area and sqm_social_zones, but we also see that there are some points that are far away from the line. We also see that there are less datapoints when sqm_social_zones > 50. It might be benefical to cut of the data there.\n",
    "\n",
    "The second plot shows that there are still a linear correlation when taking indutry into account. However the noise in the finance industry on the data becomes more visuable.\n",
    "\n",
    "We also know that netto_area = sqm_social_zones + something else. So one can create a linear model to predict the rest and then add sqm_social_zones. This is the same as forcing the weight for sqm_social_zones to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nr_employees\n",
    "In this subsection, number of employees will be explored. It makes logical sense that a higher number of employees will correlate with a higher square meter requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.lineplot(data=dataset, x='nr_employees', y='sqm_net_area', marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=dataset, x='nr_employees', y='sqm_net_area', hue='industry', marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=dataset[(dataset['nr_employees'] <= 90)], x='nr_employees', y='sqm_net_area', marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do we see?**\n",
    "\n",
    "We can see that there is a linear correlation when there are a lower number of employees, but when the number increases the noise also starts to increase. However, there are multiple points close to the extreme points, so one cannot just remove them. Moreover we can see that there is an outlier on 100 employees, this can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=dataset, x='nr_employees', y='sqm_social_zones', hue='industry', marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=dataset, x='nr_employees', y='sqm_social_zones', hue='sqm_net_area', marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do we see?**\n",
    "\n",
    "We see that a increase in both nr_employees and sqm_social_zones shows a increase in netto_area. However, we also see that a increase in nr_employees corrulate with a increase in sqm_social_zones. This also makes logical sense, so the nr_employees will be weighted in both sqm_social_zones and nr_employees, but they are both good attribute to use since most of the other attributes also are related to nr_employees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nr_closed_cubicles\n",
    "In this subsection, number of closed cubicles will be explored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=dataset, x='nr_closed_cubicles', y='sqm_net_area', marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=dataset, x='nr_closed_cubicles', y='sqm_net_area', hue='industry', marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=dataset[(dataset['nr_closed_cubicles'] < 50)], x='nr_closed_cubicles', y='sqm_net_area', hue='industry', marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do we see?**\n",
    "\n",
    "We see that there are a linear correlation between number of cubicals and netto_area regardsless of netto_area. We can see that the linear correlation breaksdown at higer nr of closed cubicles, especially in the financial industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### occupancy_meeting_room\n",
    "In this subsection, occupancy meeting room will be explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=dataset, x=\"nr_meeting_room_total_occupancy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=dataset, x='nr_meeting_room_total_occupancy', y='sqm_net_area', marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=dataset, x='nr_meeting_room_total_occupancy', y='sqm_net_area', hue='industry', marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=dataset[(dataset['nr_meeting_room_total_occupancy'] < 30)], x='nr_meeting_room_total_occupancy', y='sqm_net_area', hue='industry', marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do we see?**\n",
    "\n",
    "We can see that there a linear correlation also between occupancy_meeting_room and netto_area. However, there are some outliers on when the occupancy_meeting_rooms is above 30. These should therefore be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering & data cleaning\n",
    "\n",
    "Not all features from the dataset might be relevant for your model, and multiple features my describe the same information. It will be benefical to remove these duplicates, since the model will be weighted more towards them. \n",
    "\n",
    "Some datapoints can be combined into one more useful feature, i.e. radius from city center instead of longitude and latitude.\n",
    "\n",
    "The dataset is also not perfect. There can be missing fields. We will therefore have to clean the dataset and estimate these values. One method is to ignore the features with missing values, but then you might remove useful features. Another method is to infer them, i.e. use the mean value for the missing fields.\n",
    "\n",
    "Removing outliers will also be benefical for a linear regression model. An outlier is a datapoint that lies an abnormal distance from other datapoints. An outlier like this will cause an huge error, and will try to move the linear graph towards this abnormal point, and the model will therefore be less accurate.\n",
    "![Image of a single outlier](./img/outlier.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing columns**\n",
    "We will remove id and city as this is not relevant to the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['id', 'city']\n",
    "\n",
    "new_dataset = dataset.drop(columns_to_drop, axis=1)\n",
    "new_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing fields above a limit**\n",
    "\n",
    "We start by removing occupancy meeting room above 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.drop(new_dataset[(new_dataset['nr_meeting_room_total_occupancy'] > 30)].index, inplace=True)\n",
    "new_dataset['nr_meeting_room_total_occupancy'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove nr_closed_cubicles above 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.drop(new_dataset[(new_dataset['nr_closed_cubicles'] > 50)].index, inplace=True)\n",
    "new_dataset['nr_closed_cubicles'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove nr_employees above 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.drop(new_dataset[(new_dataset['nr_employees'] > 90)].index, inplace=True)\n",
    "new_dataset['nr_employees'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing individual points**\n",
    "\n",
    "Now we want to remove some of the points that are clear outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=new_dataset, x='sqm_social_zones', y='sqm_net_area', marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=new_dataset[new_dataset['sqm_social_zones'] > 44], x='sqm_social_zones', y='sqm_net_area', marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# finding the point\n",
    "new_dataset[(new_dataset['sqm_social_zones'] == 45)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing this point\n",
    "new_dataset.drop(65, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=new_dataset[new_dataset['sqm_social_zones'] > 40], x='sqm_social_zones', y='sqm_net_area', marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=new_dataset, x='sqm_social_zones', y='sqm_net_area', marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting categorical values to numeric\n",
    "In the provided dataset you have two categorical values, city and industry. These values cannot be used in a linear regression model since it only accepts numeric inputs. A solution to this problem is to convert categorical values to numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts tech to 1 and finance to 0\n",
    "new_dataset['industry_numeric'] = pd.Categorical(new_dataset['industry']).codes\n",
    "new_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove industry from dataset\n",
    "new_dataset.drop('industry', axis=1, inplace=True)\n",
    "new_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a machine learning model\n",
    "In this notebook, only one linear regression model will be trained and evaluated. However, when you create a machine learning model you should try with different machine learning methods and hyperparameters, such as learning rate. After trying different models you can choose the best ones and iterate on them, or combined them in a ensemble model.\n",
    "\n",
    "There is no need to develop a machine learning algorithm from scratch, unless you want to learn how it is done, since scikit learn provides you with many of the basic models out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into training and testing dataset\n",
    "training_dataset, testing_dataset = train_test_split(new_dataset, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the label from the training dataset\n",
    "training_x = training_dataset.drop(\"sqm_net_area\", axis=1)\n",
    "training_y = training_dataset[\"sqm_net_area\"]\n",
    "training_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_x = testing_dataset.drop(\"sqm_net_area\", axis=1)\n",
    "testing_y = testing_dataset[\"sqm_net_area\"]\n",
    "testing_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model = LinearRegression().fit(training_x, training_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_model.score(testing_x, testing_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do we see here?**\n",
    "\n",
    "The score shows the linear regressions accuracy on the testset. Here we can see that the model is 86.4% accurate. This is a pretty good result, but one can also improve on it by for example using a different model. The teams will probably get a different result, since the dataset has been changed since the creation of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading: Evaluate the machine learning model\n",
    "When you have multiple machine learning models, or just one, a important step will be to evaluate its performance. A common method is to have a testing dataset that is seperate from the training dataset, and check the accuracy of this. The reason one would not validate on the same training dataset is because it will mask overfitting, the production of an analysis that corresponds too closely or exactly to a particular set of data.\n",
    "\n",
    "Another method of validation is K-Fold Cross Validation. This works by dividing the dataset in to k subsets. 1 of the subsets will be used for testing while the others will be used for training. After the first validation the next subset will be used for testing and the model will be trained with the remaining subsets. The reason to choose K-Fold Cross Validation is since this will mask any imbalance in the dataset and give a more accurate picture of the models performance.\n",
    "\n",
    "A good resource to read more about sklearn's cross validation functions is their [website](https://scikit-learn.org/stable/modules/cross_validation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading: Ensemble Learning\n",
    "A widely used method for improving performance of simple machine learning algorithms is ensemble learning. It is used to combine multiple simpler models into one bigger model that performs better. There are multiple ways of combining models into one, and it is suggested that the teams look into this when they have a good basic model that they can improve upon.\n",
    "\n",
    "A good read on this topic: [A Gentle Introduction to Ensemble Learning Algorithms](https://machinelearningmastery.com/tour-of-ensemble-learning-algorithms/)\n",
    "\n",
    "![Apes stronger together meme for ensembled learning](./img/ensemble_learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![Arealize logo](./img/arealize.png)\n",
    "Arealize AI\n",
    "\n",
    "Authors: Arealize AI devteam; Petterson M., Sivarasa S., Tidemann M. & Zagorov Y."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
